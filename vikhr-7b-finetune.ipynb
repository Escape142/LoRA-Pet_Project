{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T10:36:59.855872Z","iopub.status.busy":"2024-02-24T10:36:59.854830Z","iopub.status.idle":"2024-02-24T10:36:59.864618Z","shell.execute_reply":"2024-02-24T10:36:59.863772Z","shell.execute_reply.started":"2024-02-24T10:36:59.855833Z"},"trusted":true},"outputs":[],"source":["import os\n","import datasets\n","from transformers import (\n","    AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling, \n","    MistralForCausalLM, logging\n",")\n","from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n","import torch\n","from transformers import AutoModelForCausalLM\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Enable logging for better debuggability\n","logging.set_verbosity_info()\n","# Set environment variables\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"markdown","metadata":{},"source":["### Load foundation model and tokenizer"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T10:36:59.877810Z","iopub.status.busy":"2024-02-24T10:36:59.876898Z","iopub.status.idle":"2024-02-24T10:36:59.890744Z","shell.execute_reply":"2024-02-24T10:36:59.889797Z","shell.execute_reply.started":"2024-02-24T10:36:59.877770Z"},"trusted":true},"outputs":[],"source":["train_file_path = \"/app/data/rukava_data.txt\"\n","model_name = 'Vikhrmodels/Vikhr-7B-instruct_0.4'\n","output_dir = '/app/model/lora'\n","\n","overwrite_output_dir = True\n","per_device_train_batch_size = 16\n","num_train_epochs = 3.0\n","save_steps = 2000"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["loading file tokenizer.model from cache at /app/model/models--Vikhrmodels--Vikhr-7B-instruct_0.4/snapshots/dd960329b333cbe7f67396c5fe715f477b997639/tokenizer.model\n","loading file tokenizer.json from cache at /app/model/models--Vikhrmodels--Vikhr-7B-instruct_0.4/snapshots/dd960329b333cbe7f67396c5fe715f477b997639/tokenizer.json\n","loading file added_tokens.json from cache at /app/model/models--Vikhrmodels--Vikhr-7B-instruct_0.4/snapshots/dd960329b333cbe7f67396c5fe715f477b997639/added_tokens.json\n","loading file special_tokens_map.json from cache at /app/model/models--Vikhrmodels--Vikhr-7B-instruct_0.4/snapshots/dd960329b333cbe7f67396c5fe715f477b997639/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /app/model/models--Vikhrmodels--Vikhr-7B-instruct_0.4/snapshots/dd960329b333cbe7f67396c5fe715f477b997639/tokenizer_config.json\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","loading configuration file config.json from cache at /app/model/models--Vikhrmodels--Vikhr-7B-instruct_0.4/snapshots/dd960329b333cbe7f67396c5fe715f477b997639/config.json\n","Model config LlamaConfig {\n","  \"_name_or_path\": \"Vikhrmodels/Vikhr-7B-instruct_0.4\",\n","  \"architectures\": [\n","    \"MistralForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 32768,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"sliding_window\": 4096,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.42.0.dev0\",\n","  \"use_cache\": false,\n","  \"vocab_size\": 79099\n","}\n","\n","loading weights file model.safetensors from cache at /app/model/models--Vikhrmodels--Vikhr-7B-instruct_0.4/snapshots/dd960329b333cbe7f67396c5fe715f477b997639/model.safetensors.index.json\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"use_cache\": false\n","}\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3131d95acb4a46fa8d559647bfc31098","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["All model checkpoint weights were used when initializing LlamaForCausalLM.\n","\n","All the weights of LlamaForCausalLM were initialized from the model checkpoint at Vikhrmodels/Vikhr-7B-instruct_0.4.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n","loading configuration file generation_config.json from cache at /app/model/models--Vikhrmodels--Vikhr-7B-instruct_0.4/snapshots/dd960329b333cbe7f67396c5fe715f477b997639/generation_config.json\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"use_cache\": false\n","}\n","\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"/app/model\")\n","foundation_model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=\"/app/model\")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Context size (maximum sequence length): 32768\n","Vocabulary size: 79085\n","Token: hello, ID: 21558\n","Token: world, ID: 9471\n","Token: transformers, ID: 0\n","Token: bloomz, ID: 0\n"]}],"source":["# Get the context size (maximum sequence length)\n","model_config = foundation_model.config\n","context_size = model_config.max_position_embeddings\n","print(f\"Context size (maximum sequence length): {context_size}\")\n","\n","# Get the vocabulary size\n","vocab_size = tokenizer.vocab_size\n","print(f\"Vocabulary size: {vocab_size}\")\n","\n","# Print some specific tokens and their corresponding IDs\n","specific_tokens = [\"hello\", \"world\", \"transformers\", \"bloomz\"]\n","for token in specific_tokens:\n","    token_id = tokenizer.convert_tokens_to_ids(token)\n","    print(f\"Token: {token}, ID: {token_id}\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["#this function returns the outputs from the model received, and inputs.\n","def get_outputs(model, inputs, max_new_tokens=100):\n","    outputs = model.generate(\n","        input_ids=inputs[\"input_ids\"],\n","        attention_mask=inputs[\"attention_mask\"],\n","        max_new_tokens=max_new_tokens,\n","        repetition_penalty=1.5, #Avoid repetition.\n","        early_stopping=False, #The model can stop before reach the max_length\n","        eos_token_id=tokenizer.eos_token_id\n","    )\n","    return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Inference original model\n","input_sentences = tokenizer(\"I want you to act as a motivational coach. \", return_tensors=\"pt\")\n","foundational_outputs_sentence = get_outputs(foundation_model, input_sentences, max_new_tokens=50)\n","\n","print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"]},{"cell_type":"markdown","metadata":{},"source":["### Create finetuning task"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["from transformers import BitsAndBytesConfig\n","\n","config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n","loading configuration file config.json from cache at /app/model/models--Vikhrmodels--Vikhr-7B-instruct_0.4/snapshots/dd960329b333cbe7f67396c5fe715f477b997639/config.json\n","You are using a model of type llama to instantiate a model of type mistral. This is not supported for all configurations of models and can yield errors.\n","Model config MistralConfig {\n","  \"_name_or_path\": \"output1\",\n","  \"architectures\": [\n","    \"MistralForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 32768,\n","  \"model_type\": \"mistral\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"sliding_window\": 4096,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.42.0.dev0\",\n","  \"use_cache\": false,\n","  \"vocab_size\": 79099\n","}\n","\n","loading weights file model.safetensors from cache at /app/model/models--Vikhrmodels--Vikhr-7B-instruct_0.4/snapshots/dd960329b333cbe7f67396c5fe715f477b997639/model.safetensors.index.json\n","Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"use_cache\": false\n","}\n","\n","target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f60f243bf6be4a9283523cb07caa4cab","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["All model checkpoint weights were used when initializing MistralForCausalLM.\n","\n","All the weights of MistralForCausalLM were initialized from the model checkpoint at Vikhrmodels/Vikhr-7B-instruct_0.4.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n","loading configuration file generation_config.json from cache at /app/model/models--Vikhrmodels--Vikhr-7B-instruct_0.4/snapshots/dd960329b333cbe7f67396c5fe715f477b997639/generation_config.json\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"use_cache\": false\n","}\n","\n"]}],"source":["lora_config = LoraConfig(\n","    r=128,\n","    target_modules=\"all-linear\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","model = MistralForCausalLM.from_pretrained(model_name,\n","                                           quantization_config=config,\n","                                           torch_dtype=torch.bfloat16,\n","                                           device_map=\"auto\",\n","                                           low_cpu_mem_usage=True,\n","                                           trust_remote_code=True,\n","                                           cache_dir=\"/app/model\")\n","# model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","model.gradient_checkpointing_enable()\n","\n","model = prepare_model_for_kbit_training(model)\n","\n","model = get_peft_model(model,lora_config)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T10:40:55.799266Z","iopub.status.busy":"2024-02-24T10:40:55.798509Z","iopub.status.idle":"2024-02-24T10:40:55.814312Z","shell.execute_reply":"2024-02-24T10:40:55.813228Z","shell.execute_reply.started":"2024-02-24T10:40:55.799231Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 335,544,320 || all params: 7,963,111,424 || trainable%: 4.2137\n"]}],"source":["model.print_trainable_parameters()"]},{"cell_type":"markdown","metadata":{},"source":["### Load data"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["examples_dir = os.path.join(\"/app/data\")\n","\n","def load_example(filename):\n","    with open(os.path.join(examples_dir, filename) , 'r', encoding='utf-8') as f:\n","        return f.read()\n","\n","data = load_example(\"rukava_data.txt\")"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["x = \"\"\"Human: РВД D19(20) P350 4SP JIC G1,5/16 L3000 DIN EN856\n","Assistant:\n","Простая/составная - Составная\n","Продукт/рукав - 4SP DN19\n","Длина - 3000\n","Фитинг левый - JIC1.5/16\"DN19\n","Фитинг правый - JIC1.5/16\"DN19\"\"\""]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["def tokenize_training_text(training_text, max_seq_length, tokenizer, separator=\"\\n\\n\\n\", **kwargs):\n","    samples_from = training_text.split(separator)\n","    \n","    samples_to=[]\n","    # samples = [x.strip() for x in samples]\n","\n","    # def to_dict(text):\n","    #     return {'text': text}\n","\n","    # samples = [to_dict(x) for x in samples]\n","    for x in samples_from:\n","        human_part, assistant_part = x.split(\"Assistant:\")\n","        human_part.replace(\"Human: \", \"\")\n","        assistant_text = assistant_part.strip()\n","        samples_to.append({\"human\": human_part, \"assistant\": assistant_text})\n","        # for assistant_line in assistant_text.split(\"\\n\"):\n","        #     key, value = assistant_line.split(\" - \")\n","            \n","        # samples_to.append(data)\n","    training_dataset = datasets.Dataset.from_list(samples_to)\n","    print(training_dataset)\n","    training_dataset = training_dataset.shuffle().map(\n","        lambda x: tokenize_sample(x, max_seq_length, tokenizer), \n","        batched=False\n","    )\n","\n","    return training_dataset\n","    \n","def tokenize_sample(item, max_seq_length, tokenizer, add_eos_token=True):\n","    assert tokenizer is not None\n","    result = tokenizer(\n","        item[\"text\"],\n","        truncation=True,\n","        max_length=max_seq_length,\n","        padding=\"max_length\",\n","    ) \n","\n","    if add_eos_token and (len(result[\"input_ids\"]) < max_seq_length or result[\"input_ids\"][-1] != tokenizer.eos_token_id):\n","        result[\"input_ids\"].append(tokenizer.eos_token_id)\n","        result[\"attention_mask\"].append(1)\n","\n","    return result\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T10:36:59.892070Z","iopub.status.busy":"2024-02-24T10:36:59.891777Z","iopub.status.idle":"2024-02-24T10:38:22.057056Z","shell.execute_reply":"2024-02-24T10:38:22.055867Z","shell.execute_reply.started":"2024-02-24T10:36:59.892031Z"},"trusted":true},"outputs":[],"source":["def load_dataset(file_path, tokenizer, block_size=512):\n","    dataset = datasets.load_dataset('text', data_files=file_path)\n","    dataset = dataset['train']\n","    dataset = dataset.map(\n","        lambda e: tokenizer(e['text'], truncation=True, padding='max_length', max_length=block_size),\n","        batched=True\n","    )\n","    return dataset\n","\n","\n","def load_data_collator(tokenizer, mlm=False):\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer,\n","        mlm=mlm,\n","        \n","        pad_to_multiple_of=8, \n","    )\n","    return data_collator"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-02-24T10:40:59.297785Z","iopub.status.busy":"2024-02-24T10:40:59.297393Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"450e942ac2c74f1582b7fb78dd81ce7d","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2866 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_dataset = tokenize_training_text(training_text=data, max_seq_length=1024, tokenizer=tokenizer)\n","data_collator = load_data_collator(tokenizer)"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"data":{"text/plain":["{'text': 'Human: РУКАВ НЕМЕТАЛЛИЧЕСКИЙ В СБОРЕ_12-4SP-2DKOS-(R)- M24X1.5-5600_ВЫСОКОГО ДАВЛЕНИЯ _DN12 мм_5600 мм \\nAssistant: \\nПростая/составная - Составная\\nПродукт/рукав - 4SP DN12\\nДлина - 5600\\nФитинг левый - DKOS24x1,5DN12\\nФитинг правый - DKOS24x1,5DN12',\n"," 'input_ids': [2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  2,\n","  10649,\n","  28747,\n","  43436,\n","  34435,\n","  28849,\n","  38571,\n","  74652,\n","  50667,\n","  35079,\n","  38034,\n","  40666,\n","  1519,\n","  43452,\n","  36050,\n","  28907,\n","  28730,\n","  28740,\n","  28750,\n","  28733,\n","  28781,\n","  3701,\n","  28733,\n","  28750,\n","  28757,\n","  28796,\n","  3843,\n","  24894,\n","  28754,\n","  9572,\n","  351,\n","  28750,\n","  28781,\n","  28814,\n","  28740,\n","  28723,\n","  28782,\n","  28733,\n","  28782,\n","  28784,\n","  28734,\n","  28734,\n","  28730,\n","  54938,\n","  36682,\n","  34046,\n","  33731,\n","  54692,\n","  56896,\n","  39367,\n","  583,\n","  28385,\n","  28740,\n","  28750,\n","  34336,\n","  28730,\n","  28782,\n","  28784,\n","  28734,\n","  28734,\n","  34336,\n","  28705,\n","  13,\n","  7226,\n","  11143,\n","  28747,\n","  28705,\n","  13,\n","  33274,\n","  36413,\n","  28748,\n","  2688,\n","  6126,\n","  6075,\n","  387,\n","  47538,\n","  6075,\n","  13,\n","  33274,\n","  2454,\n","  6299,\n","  28748,\n","  1728,\n","  917,\n","  28791,\n","  387,\n","  28705,\n","  28781,\n","  3701,\n","  384,\n","  28759,\n","  28740,\n","  28750,\n","  13,\n","  28868,\n","  23067,\n","  387,\n","  28705,\n","  28782,\n","  28784,\n","  28734,\n","  28734,\n","  13,\n","  39822,\n","  39038,\n","  72208,\n","  387,\n","  384,\n","  28796,\n","  3843,\n","  28750,\n","  28781,\n","  ...],\n"," 'attention_mask': [0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  ...]}"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset.__getitem__(2)"]},{"cell_type":"code","execution_count":40,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n","Using auto half precision backend\n","The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text. If text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n","***** Running training *****\n","  Num examples = 2,866\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 540\n","  Number of trainable parameters = 335,544,320\n","Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]}],"source":["training_args = TrainingArguments(\n","      output_dir=output_dir,\n","      overwrite_output_dir=overwrite_output_dir,\n","      per_device_train_batch_size=per_device_train_batch_size,\n","      num_train_epochs=num_train_epochs,\n","      fp16=True,\n","      save_steps=save_steps,\n","      optim=\"adamw_bnb_8bit\",\n","  )\n","\n","trainer = Trainer(\n","      model=model,\n","      args=training_args,\n","      data_collator=data_collator,\n","      train_dataset=train_dataset,\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["You are using a model of type llama to instantiate a model of type mistral. This is not supported for all configurations of models and can yield errors.\n"]},{"data":{"text/plain":["('/app/model/lora/trained_model/tokenizer_config.json',\n"," '/app/model/lora/trained_model/special_tokens_map.json',\n"," '/app/model/lora/trained_model/tokenizer.model',\n"," '/app/model/lora/trained_model/added_tokens.json',\n"," '/app/model/lora/trained_model/tokenizer.json')"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# Save the model and the LoRA adapter\n","model_path = os.path.join(output_dir, \"trained_model\")\n","model.save_pretrained(model_path)\n","tokenizer.save_pretrained(model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the trained model and adapter\n","model = MistralForCausalLM.from_pretrained(model_name,\n","                                           quantization_config=config,\n","                                           torch_dtype=torch.bfloat16,\n","                                           device_map=\"auto\",\n","                                           low_cpu_mem_usage=True,\n","                                           trust_remote_code=True,\n","                                           cache_dir=\"/app/model\")\n","tokenizer = AutoTokenizer.from_pretrained(model_path)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"ename":"OSError","evalue":"load_adapter is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m/opt/conda/envs/peft/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/envs/peft/lib/python3.8/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n","\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/load_adapter/resolve/main/adapter_config.json","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)","File \u001b[0;32m/opt/conda/envs/peft/lib/python3.8/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/envs/peft/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/envs/peft/lib/python3.8/site-packages/huggingface_hub/file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/envs/peft/lib/python3.8/site-packages/huggingface_hub/file_download.py:1325\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/envs/peft/lib/python3.8/site-packages/huggingface_hub/file_download.py:1823\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1821\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1822\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1823\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1824\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1825\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n","File \u001b[0;32m/opt/conda/envs/peft/lib/python3.8/site-packages/huggingface_hub/file_download.py:1722\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1722\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n","File \u001b[0;32m/opt/conda/envs/peft/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/envs/peft/lib/python3.8/site-packages/huggingface_hub/file_download.py:1645\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1645\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1654\u001b[0m hf_raise_for_status(r)\n","File \u001b[0;32m/opt/conda/envs/peft/lib/python3.8/site-packages/huggingface_hub/file_download.py:372\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 372\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/envs/peft/lib/python3.8/site-packages/huggingface_hub/file_download.py:396\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    395\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n","File \u001b[0;32m/opt/conda/envs/peft/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:352\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    344\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    351\u001b[0m     )\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-665da495-1d2a572c60c774037927c7c1;9afa286d-ed1e-4a9b-8711-17ec244b80c7)\n\nRepository Not Found for url: https://huggingface.co/load_adapter/resolve/main/adapter_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_adapter\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mload_adapter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madapter_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/envs/peft/lib/python3.8/site-packages/transformers/integrations/peft.py:176\u001b[0m, in \u001b[0;36mPeftAdapterMixin.load_adapter\u001b[0;34m(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, adapter_kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     token \u001b[38;5;241m=\u001b[39m adapter_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     adapter_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mfind_adapter_config_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpeft_model_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m adapter_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    184\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madapter model file not found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_model_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Make sure you are passing the correct path to the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madapter model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    186\u001b[0m         )\n","File \u001b[0;32m/opt/conda/envs/peft/lib/python3.8/site-packages/transformers/utils/peft_utils.py:88\u001b[0m, in \u001b[0;36mfind_adapter_config_file\u001b[0;34m(model_id, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, _commit_hash)\u001b[0m\n\u001b[1;32m     86\u001b[0m         adapter_cached_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_id, ADAPTER_CONFIG_NAME)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     adapter_cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mADAPTER_CONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m adapter_cached_filename\n","File \u001b[0;32m/opt/conda/envs/peft/lib/python3.8/site-packages/transformers/utils/hub.py:425\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    421\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    433\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n","\u001b[0;31mOSError\u001b[0m: load_adapter is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"]}],"source":["if hasattr(model, 'load_adapter'):\n","    model.load_adapter('load_adapter', adapter_name=\"adapter_model\")\n","else:\n","    print(\"No\")\n","    \n","    # self.model = peft.PeftModel.from_pretrained(self.model, lora_name, adapter_name=lora_name, cache_dir=CACHE_DIR)\n","            "]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Setup the text generation pipeline\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m text_generator \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)  \u001b[38;5;66;03m# Adjust device as needed\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Function to generate text\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_text\u001b[39m(prompt, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m):\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["from transformers import pipeline \n","\n","# Setup the text generation pipeline\n","text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)  # Adjust device as needed\n","\n","# %%\n","# Function to generate text\n","def generate_text(prompt, max_length=500):\n","    return text_generator(prompt, max_length=max_length, num_return_sequences=1)\n","\n","# Example usage\n","prompt = \"Human: РВД D19(20) P350 4SP JIC G1,5/16 L3000 DIN EN856\\nAssistant:\"\n","generated_text = generate_text(prompt)\n","print(generated_text)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = get_peft_model(model, lora_config)\n","model.load_adapter(output_dir, adapter_name=\"adapter_model\")\n","\n","# Tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"/app/model\")\n","tokenizer.pad_token = tokenizer.eos_token\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to generate a response\n","def generate_response(model, tokenizer, input_text):\n","    inputs = tokenizer(input_text, return_tensors=\"pt\")\n","    outputs = model.generate(**inputs, max_new_tokens=50)\n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return response\n","\n","# Example test input\n","test_input = \"Human: 000000001300446262 Рукав высокого давления DIN 2SN-250-1000-DKOL-M26х1,5 Dв16мм L1000мм PN250бар с двумя армир.оплётками DKOL М26х1,5 DKOL м26х1,590гр., г/масло\\nAssistant:\"\n","\n","# Generate response using the model with the trained adapter\n","response = generate_response(model_with_adapter, tokenizer, test_input)\n","print(response)\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":3037859,"sourceId":5221510,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":4}
